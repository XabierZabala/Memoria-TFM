%----------------------------------------------------%
%          ENTORNO DE PRUEBAS: CONSTRUCCION          %
%----------------------------------------------------%

\pagestyle{fancy}

\chapter{Entorno de pruebas: Construcción}
\label{entorno_pruebas}

Conocedores de la problemática a solventar y de las tecnologías que se desean utilizar para ello, es momento de detallar la confección de los entornos de prueba utilizados para medir el rendimiento de dichas tecnologías.\\

En las siguientes páginas, se describe, por un lado, la construcción de un entorno centralizado compuesto por un único nodo que alberga instancia de MySQL Server, cuyo objetivo es imitar la infraestructura monolítica que impera en Datik. Por el otro lado, se expone el ensamblaje de un entorno distribuido, formado por tres nodos homogéneos sobre los cuales operan Apache Cassandra y Apache Spark en consonancia.\\

Dichos nodos son erigidos mediante máquinas virtuales que operan sobre distintos dispositivos. Los procesos clientes se alojan en un ordenador portátil mientras que las instancias relativas a los servidores se ejecutan sobre un ordenador de sobremesa, logrando así neutralizar toda diferencia que pudiese existir en cuanto a hardware. Para dar comienzo a este tercer capítulo, se describen las características físicas de los dispositivos recién mencionados.

\clearpage

\section{Características de las máquinas físicas}

\textbf{Ordenador portátil}: Terminal que alberga los procesos cliente encargados de lanzar diversas peticiones contra el servidor y recolectar los resultados computados por este, además de cuantificar el tiempo transcurrido entre ambos eventos.

\begin{itemize}
	\item Procesador: Intel(R) Core(TM) i-5 4200M CPU @ 2.50 GHz
	\item Memoria RAM: 8 GB
	\item Disco Duro: Western Digital WD5000LPCX-24C6HTO SSD 500GB
	\item Sistema Operativo: Windows 10
	\item Adaptador Red: Qualcomm Atheros AR8172/8176/8178 PCI-E Fast Ethernet Controller (NDIS 6.30)
\end{itemize}

\textbf{Ordenador de sobremesa}: Terminal que adquiere el rol de servidor dentro del entramado. Se encarga del almacenamiento y procesamiento de los datos mediante el uso de las tecnología que se analizan en el presente proyecto.

\begin{itemize}
	\item Procesador: AMD FX(tm)-8350 Eight-Core Processor 4.00GHz
	\item Memoria RAM: 32 GB
	\item Disco Duro: Western Digital WD5000AAKX-22ERMA0 SSD 500GB
	\item Sistema Operativo: Windows 10
	\item Adaptador Red: Killer e2200 Gigabit Ethernet Controller (NDIS 6.30)
\end{itemize}

Ambas máquinas se encuentran enlazadas a un \textit{switch} tp-link-tl-sf1008d mediante sendos conectores Fast Ethernet, permitiendo intercambiar datos a una velocidad de 100 Mb/s.\\

El switch, a su vez, está conectado a un \textit{router} que proporciona acceso al exterior de la red privada, el cual sólo será utilizado para descargar contenido relativo a los preparativos de las pruebas.

\clearpage

\section{Confección de las máquinas virtuales}
 
Con el objetivo de no realizar las mismas configuraciones reiteradamente, se ha decidido crear una máquina virtual básica siguiendo los pasos que se describen a continuación y tras su puesta a punto, clonarla tantas veces como instancias hagan falta.

\begin{enumerate}
	
	\item \textbf{Crear la máquina virtual}: Instancia creada mediante VirtualBox utilizando la imagen de Ubuntu Server 16.04 LTS y 75 GB de memoria física. El resto de las características no han sido especificadas ya que es posible reasignar su valor en cualquier momento.
	
	\item \textbf{Asignar una IP estática}: La mayoría de los clones generados sobre esta máquina adquieren el rol de servidor, por lo que no es de recibo que su IP pueda variar cada vez que el servicio de red o la máquina misma se reinicien. Para evitar cualquier inconveniente de este estilo, es totalmente necesario configurar una interfaz de red con una IP estática.
	
	\item \textbf{Cambiar el adaptador de red a modo puente}: Al crear una máquina virtual con VirtualBox, el adaptador de red predefinido es el NAT. Ello implica que dicha máquina se encontrará aislada en una red lógica y solo podrá acceder a otros dispositivos pasando por la máquina física y haciendo uso de su IP.\\
	
	Modificando el adaptador de red al modo puente se consigue extender la red privada hasta el nodo virtual, eliminando así toda dependencia con la máquina física y posibilitando la comunicación con el resto de los dispositivos que residen en la misma red privada.
	
	\item \textbf{Instalar Java 8}: Las tecnologías que se desean probar en las futuras pruebas se ejecutan sobre la Máquina Virtual de Java(JVM), por lo que es necesario instalar una versión de Java para su funcionamiento. Entre todas las disponibles se ha optado por la versión 8.
	
\end{enumerate}

Una vez terminado de confeccionar la máquina virtual básica, es aconsejable ejecutar el comando \textit{tracert} y comprobar ciertas métricas como la latencia y el número de saltos que se dan por la red hasta llegar a otro nodo de la infraestructura. Siguiendo este simple consejo es posible identificar errores relacionados con la configuración y solucionarlos con mayor celeridad.

\clearpage

\section{Entorno centralizado}

El objetivo del entorno centralizado es imitar la infraestructura utilizada por Datik para ejecutar el proceso Cálculo de Indicadores. Dicha infraestructura consta de un servidor en la nube que alberga una instancia de MySQL Server y un proceso que actúa como cliente. Este último realiza consultas contra la base de datos y computa los registros obtenidos para calcular los indicadores deseados.

\subsection{Servidor}

La creación y confección del nodo servidor para el entorno de pruebas centralizado se resume en los siguientes pasos:

\begin{enumerate}
	
\item \textbf{Clonar la máquina virtual base}: Crear una copia de la instancia básica confeccionada en el apartado anterior y asignar los parámetros especificados en cuadro \ref{nodo-mysql} de la página \pageref{nodo-mysql}.

\begin{table}[h!]
	\centering
	\begin{tabular}{|l||l|l|l|}
		
		\hline
		
		& \textbf{IP Privada} & \textbf{Procesadores} & \textbf{Memoria RAM} \\
		
		\hline
		\hline
		
		Nodo MySQL & 192.168.0.100 & 6 & 24 GB \\
		
		\hline
		
	\end{tabular}
	\caption{Descripción del nodo centralizado}
	\label{nodo-mysql}
\end{table}

\item \textbf{Descargar e instalar la última versión de MySQL}: Acceder a la máquina virtual recientemente clonada e instalar la última versión de MySQL Server, la 5.7, mediante Advanced Packaging Tool (APT) de Ubuntu.

\item \textbf{Crear un esquema}: Identificarse contra MySQL Server utilizando el único usuario existente por el momento, 'root', y la contraseña que se le ha asignado durante la instalación para crear el esquema encargado de almacenar los datos.

\item \textbf{Posibilitar acceso desde la máquina remota}:  Por motivos de seguridad, las aplicaciones que se conectan a la base de datos no deberían de acceder a ella mediante el usuario 'root', por lo que se recomienda crear un nuevo usuario y especificar los permisos que este ha de tener sobre el esquema creado en el paso anterior.

\item \textbf{Abrir los puertos necesarios}: Al estar trabajando sobre máquinas virtuales alojadas en una red propia, todos los puertos se encuentran abiertos por defecto, pero en caso de existir algún cortafuegos que protegiera el servidor, sería necesario abrir el acceso al puerto donde escucha MySQL, el 3306 por defecto.

\end{enumerate}

\subsection{Emulando el cliente}

Emular un proceso cliente en el entorno centralizado es tan simple como instalar la última versión del MySQL Workbench\footnote{\url{https://www.mysql.com/products/workbench/}} sobre el ordenador portátil (no hace falta máquina virtual alguna) y crear una nueva conexión con el servidor especificando la IP junto al usuario y contraseña correspondientes.

\section{Entorno distribuido}

Infraestructura cuyo núcleo es un clúster compuesto por tres nodos virtuales alojados en el ordenar de sobremesa erigido con el objetivo de almacenar y procesar el dataset de prueba.\\ 

Un proceso cliente residente en el ordenar portátil, por su parte, se encarga de ejecutar el programa que especifica el procesamiento que ha de llevar a cabo el clúster y cuantificar el tiempo transcurrido en dicha tarea.

\subsection{Servidor distribuido}

En las siguientes líneas se detalla la configuración de los nodos que conforman el clúster y la puesta a punto de las tecnologías que operan sobre él.

\begin{enumerate}
	
\item \textbf{Clonar la máquina virtual base}: Clonar la máquina virtual base tantas veces como nodos vayan a conformar la infraestructura. En éste particular caso se crean 3 copias.\\

Además de configurar la IP, el número de procesadores y la memoria RAM disponible para cada uno, es necesario hacer lo propio con el hostname. Apache Cassandra recurre a este atributo para conocer la IP de una máquina cuando ciertos elementos del fichero de configuración no son especificados y en el caso de Spark, el proceso \textit{driver} utiliza los hostname para establecer la conexión con otros nodos de la infraestructura. El cuadro \ref{nodos-cluster} de la página \pageref{nodos-cluster} resume la asignación de dichos atributos.\\

\begin{table}[h!]
	\centering
	\begin{tabular}{|l||l|l|l|l|}
		
		\hline
		
		& \textbf{IP Privada} & \textbf{Procesadores} & \textbf{Memoria RAM} & \textbf{Hostname} \\
		
		\hline
		\hline
		
		Nodo 1 & 192.168.0.101 & 2 & 8 GB & nodo1 \\
		
		\hline
		
		Nodo 2 & 192.168.0.102 & 2 & 8 GB & nodo2 \\
		
		\hline
		
		Nodo 3 & 192.168.0.103 & 2 & 8 GB & nodo3 \\
		
		\hline
		
	\end{tabular}
	\caption{Descripción de los nodos que componen el clúster}
	\label{nodos-cluster}
\end{table}

Cabe destacar que la suma de recursos físicos destinados a la creación del clúster es idéntica a los recursos destinados para erigir el servidor del entorno centralizado.

\item \textbf{Resolver hostnames a IPs}: Los hostname recién configurados no sirven de nada si no es posible asociarlos a una IP concreta. Dicho trabajo lo suele realizar un DNS, pero al estar operando sobre una red privada que no dispone de DNS alguno, es necesario recurrir al fichero \textit{/etc/hosts} para obtener la información deseada. Este fichero debe de albergar la IP privada y el hostname de la propia máquina y del resto de hosts que conforman el clúster.

\item \textbf{Descargar Apache Cassandra}: Puede ser instalada mediante paquetes deb o rpm pero en este proyecto se ha optado por descargar el archivo binario y ejecutarlo de forma manual una vez realizadas las configuraciones pertinentes.\\ 

Siendo un proyecto de código abierto relativamente reciente, las nuevas versiones y actualizaciones son lanzadas frecuentemente para arreglar bugs e implementar funcionalidades nuevas. Debido a ello, es imposible trabajar con la última versión, por lo que se ha decidido utilizar la 2.1.8 para el desarrollo del presente proyecto.

\item \textbf{Descargar Apache Spark}: Es necesario tener en cuenta la compatibilidad con la versión de Cassandra ya descargada. Aunque en principio se trata de dos proyectos totalmente independientes, existen conectores que buscan facilitar un uso conjunto de estas tecnologías. Analizando la tabla de compatibilidades\footnote{\url{https://github.com/datastax/spark-cassandra-connector}} ofrecida por DataStax, dueña del conector seleccionado para el presente proyecto, se ha decidido descargar la versión 1.4.1 de Spark.
	
\end{enumerate}

\subsubsection{Configuración de Apache Cassandra}

El fichero que alberga todos los atributos configurables de Cassandra es el denominado  \textit{cassandra.yaml} y se encuentra alojado dentro de la carpeta \textit{conf}. Es necesario modificar en cada uno de los nodos que conforman el clúster, los atributos que se definen a continuación:

\begin{itemize}

\item \textbf{cluster\_name}: Varios clúster lógicos pueden coexistir dentro de uno físico. Éste atributo permite determinar a cual de todos pertenece el nodo.

\item \textbf{initial\_token y num\_token}: Al insertar un registro, un proceso denominado \textit{partitioner}\footnote{\url{https://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architecturePartitionerAbout_c.html}} se encarga de dilucidar en cual de los nodos se ha de almacenar. Para ello, cada nodo tiene que conocer de antemano el rango de particiones que se le han atribuido, especificado mediante el atributo initial\_token.\\

No obstante, remover o añadir nuevos nodos en el sistema implica volver a calcular y modificar dicho rango en cada máquina; tarea inviable cuando la envergadura del clúster aumenta considerablemente. Para automatizar las operaciones mencionadas, las versiones más reciente de Cassandra añaden el concepto de nodos virtuales\footnote{\url{http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeVnodesUsing_c.html}} y la opción de activar dicha funcionalidad mediante el atributo num\_token.

\item \textbf{seed\_provider}: Lista de direcciones IP delimitadas por coma que se utilizan como punto de contacto cuando un nodo desea unirse al clúster.

\item \textbf{listen\_address}: La dirección IP que utilizan otros nodos para conectarse a una máquina especifica. Si no se indica nada, el nombre de la máquina tiene que conducir a su IP utilizando el fichero \textit{etc/hostname}.

\item \textbf{rpc\_address}: La dirección IP de escucha para conexiones de cliente. Sus valores posibles son:

\begin{itemize}
	\item 0.0.0.0: Escucha en todas la interfaces configuradas
	\item Dirección IP
	\item Nombre de máquina
	\item Sin especificar: el nombre de la máquina tiene que conducir a su IP utilizando el fichero \textit{etc/hostname}
\end{itemize}

\item \textbf{endpoint\_snitch}: Establece el modo en el que Cassandra localiza nodos y envía peticiones de enrutamiento. Todos los nodos han de tener el mismo valor, siendo los siguiente los más utilizados:

\begin{itemize}
	\item SimpleSnitch: Se utiliza solo para la implementación de centro de datos únicos.
	\item RackInferredSnitch: Determina la ubicación de los nodos por rack o por data center.
\end{itemize}
	 
\end{itemize}
	 
\begin{table}[h!]
	\centering
	\begin{tabular}{|l||l|l|l|}
		
		\hline
		
		\textbf{Atributo} & \textbf{Nodo} 1 & \textbf{Nodo 2} & \textbf{Nodo 3} \\
		
		\hline
		\hline
		
		cluster\_name & Test Cluster & Test Cluster & Test Cluster \\
		
		\hline
		
		num\_token & 256 & 256 & 256 \\
		
		\hline
		
		seed\_provider & 192.168.0.102 & 192.168.0.101 & 192.168.0.101 \\ 
		               & 192.168.0.103 & 192.168.0.103 & 192.168.0.102 \\ 
		
		\hline
		
		listen\_address & 192.168.0.101 & 192.168.0.102 & 192.168.0.103 \\
		
		\hline
		
		rpc\_address & 192.168.0.101 & 192.168.0.102 & 192.168.0.103 \\
		
		\hline
		
		endpoint\_snitch & SimpleSnitch & SimpleSnitch & SimpleSnitch \\
		
		\hline
		
	\end{tabular}
	\caption{Configuración de los nodos Cassandra}
	\label{configuracion-nodos-cassandra}
\end{table}

Una vez habiendo configurado cada nodo de la forma indica en el cuadro \ref{configuracion-nodos-cassandra} de la página \pageref{configuracion-nodos-cassandra}, es necesario abrir los puertos detallados en el cuadro \ref{puertos-cassandra} de la página \pageref{puertos-cassandra} para que los procesos Cassandra puedan comunicarse entre los diferentes nodos que conforman el clúster.\\

\begin{table}[h!]
	\centering
	\begin{tabular}{|l||l|l|l|}
		
		\hline
		
		\textbf{Puerto} & \textbf{Descripción} \\
		
		\hline
		\hline
		
		7000 & Comunicación entre nodos utilizado en caso de no tener activado TLS \\
		
		\hline
		
		7001 & Comunicacion TLS entre nodos \\
		
		\hline
		
		7199 & Puerto para acceder a JMX \\ 
		
		\hline
		
		9042 & Puerto nativo de transporte para CQL \\
		
		\hline
		
		9160 & Cliente del API Thrift \\
		
		\hline
		
	\end{tabular}
	\caption{Puertos utilizados por Cassandra}
	\label{puertos-cassandra}
\end{table}

Para finalizar, es necesario arrancar los nodos uno a uno mediante el script \textit{cassandra.sh} que Cassandra mismo facilita dentro de la carpeta \textit{/bin} y comprobar que el clúster funciona correctamente utilizando la herramienta \textit{nodetool}\footnote{\url{http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/tools/toolsNodetool_r.html}}.

\subsubsection{Configuración de Apache Spark}

Spark utiliza la memoria RAM de forma intensiva, por lo que se recomienda encarecidamente instalarlo en un clúster totalmente aislado al de Cassandra. No obstante, debido a que los recursos disponibles para lleva adelante el presente proyecto son limitados, no se ha podido seguir la recomendación y Spark ha sido instalado de tal forma que ambas tecnologías comparten nodo.\\

Al tratarse de un entorno de pruebas donde la cantidad de datos a manipular se encuentra predefinida, la instalación realizada no supone problema alguno y además ofrece diversas ventajas como el no tener que volver a configurar nuevas máquinas dentro del entramado, pudiendo pasar directamente a especificar las configuraciones relativas a Spark:

\clearpage

\begin{enumerate}
	
\item \textbf{Definir nodos esclavos}: El nodo maestro de Spark necesita saber qué máquinas están dispuestas a ofrecer sus recursos para un futuro procesamiento y poder alojar en ellas los \textit{worker} pertinentes. Para ello es necesario especificar las IP de dichos nodos en el fichero \textit{/conf/slaves}. Se ha de crear el directorio en caso de que no exista.

\item \textbf{Acceso entre máquinas mediante las claves RSA}: Spark sigue una arquitectura maestro/esclavo donde el maestro necesita conectarse a los esclavos para comunicar las operaciones que estos han de ejecutar. Para automatizar el proceso y evitar tener que introducir la contraseña cada vez que se quiera ejecutar algo, es necesario garantizar el acceso entre máquinas mediante las claves RSA.\\

Se recomienda crear los certificados con el comando \textit{ssh-keygen} y distribuirlos a los nodos cuya IP haya sido especificada en el fichero \textit{/conf/slaves} mediante el comando \textit{ssh-copy-id {usuario}@{servidor}} especificando el usuario que va a ejecutar el proceso Spark.

\item \textbf{Resolver hostnames a IPs}: Al instalar Cassandra se ha realizado el presente paso por lo que no hace falta volver a repetirlo. Aún así, cabe recordar que en caso de instalar Spark en un clúster independiente sería necesario manipular el fichero \textit{/etc/hosts} para especificar la IP privada y el hostname de la propia máquina y del resto de hosts que conforman el clúster.

\item \textbf{Definir el nodo Maestro}: Los nodos esclavos necesitan conocer la IP del terminal que adquiere el rol de maestro para poder aceptar las tareas enviadas por este último. Ello se consigue modificando  el fichero \textit{conf/spark-env.sh} y añadiendo las líneas \textit{SPARK\_MASTER\_HOST=ip\_del\_host} y \textit{SPARK\_LOCAL\_IP=ip\_del\_host} en cada una de ellas.
	
\end{enumerate}

Antes de iniciar el proceso es necesario cerciorarse de que los puertos mostrados en el cuadro \ref{puertos-spark} de la página \pageref{puertos-spark} se encuentran abiertos y disponibles para su uso:\\

\begin{table}[h!]
	\centering
	\begin{tabular}{|l||l|l|l|}
		
		\hline
		
		\textbf{Puerto} & \textbf{Descripción} \\
		
		\hline
		\hline
		
		8080 & Interfaz web para visualizar métricas relativas al maestro  \\
		
		\hline
		
		8081 & Interfaz web para visualizar métricas relativas a los workers \\
		
		\hline
		
		7077 & Conexión con el clúster y envío de tareas \\ 
		
		\hline
		
		aleatorio & Puerto asignado a cada worker creado para la computación \\
		
		\hline
		
	\end{tabular}
	\caption{Puertos utilizados por Spark}
	\label{puertos-spark}
\end{table}

Para finalizar, solo hace falta lanzar el proceso Spark mediante el script \textit{start-all} disponible en la carpeta \textit{sbin}.

\clearpage

\subsection{Cliente}

Para emular el cliente que interacciona con el clúster del entorno distribuido se ha optado por crear una máquina virtual sobre el ordenador portátil y configurar dentro de dicha instancia los elementos necesarios para lanzar los procesos.

\begin{enumerate}
	
	\item \textbf{Clonar la máquina virtual base}: Crear una copia de la instancia básica y asignar los parámetros que aparecen en cuadro \ref{nodo-cliente-cluster} de la página \pageref{nodo-cliente-cluster}.
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|l||l|l|l|}
			
			\hline
			
			& \textbf{IP Privada} & \textbf{Procesadores} & \textbf{Memoria RAM} \\
			
			\hline
			\hline
			
			Nodo Cliente & 192.168.0.104 & 2 & 4 GB \\
			
			\hline
			
		\end{tabular}
		\caption{Descripción del nodo cliente en el entorno distribuido}
		\label{nodo-cliente-cluster}
	\end{table}
	
	\item \textbf{Instalar Eclipse}: El entorno de desarrollo integrado seleccionado para ejecutar el proceso cliente es Eclipse Luna.
	
	\item \textbf{Instalar Maven}: Herramienta de software para la gestión y construcción de proyectos Java que facilita de sobremanera la gestión de las librerías y sus respectivas dependencias. Instalado mediante Advanced Packaging Tool (APT) de Ubuntu.

	\item \textbf{Crear un proyecto Java de Maven e incluir dependencias}: Eclipse ofrece la posibilidad de crear un proyecto Java con naturaleza de Maven y así poder insertar las siguientes librerías y sus respectivas dependencias cómodamente:
	
	\begin{itemize}
		\item \textbf{Cassandra Driver Core} \footnote{\url{https://mvnrepository.com/artifact/com.datastax.cassandra/cassandra-driver-core/2.1.8}}: Contiene los métodos necesarios para interaccionar con Cassandra mediante Java.
		\item \textbf{Spark Core} \footnote{\url{https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10}}: Librería que permite utilizar las funcionalidades centrales de Spark mediante Java.
		\item \textbf{Connector Spark-Cassandra} \footnote{\url{https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10}}: Librería que ofrece funcionalidades para un uso conjunto de Spark y Cassandra  
	\end{itemize}
	
\end{enumerate}